# ITË©¶È®ìÂ≠¶Áøí„Ç∑„Çπ„ÉÜ„É† - „Ç∑„Çπ„ÉÜ„É†Ë®≠Ë®à„ÉªÈñãÁô∫ÈÅãÁî®ÊîπÂñÑÊèêÊ°àÊõ∏

**Á≠ñÂÆöÊó•**: 2025Âπ¥7Êúà30Êó•
**„Éó„É≠„Ç∏„Çß„ÇØ„Éà**: ÊÉÖÂ†±ÊäÄË°ìËÄÖË©¶È®ìÂ≠¶Áøí„Ç∑„Çπ„ÉÜ„É† ÂåÖÊã¨ÁöÑÊîπÂñÑ
**ÂØæË±°**: „Ç∑„Çπ„ÉÜ„É†Ë®≠Ë®à„ÉªÈñãÁô∫„ÉªÈÅãÁî®„Éó„É≠„Çª„ÇπÊîπÂñÑ

---

## üèóÔ∏è Ê¨°‰∏ñ‰ª£„Ç∑„Çπ„ÉÜ„É†„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£

### Current vs Target Architecture

#### **ÁèæÂú®„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÔºàÊîπÂñÑÊ∏à„ÅøÔºâ**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ   Backend       ‚îÇ    ‚îÇ   Database      ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Jinja2        ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ Flask         ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ SQLite        ‚îÇ
‚îÇ ‚Ä¢ Bootstrap     ‚îÇ    ‚îÇ ‚Ä¢ Blueprint     ‚îÇ    ‚îÇ ‚Ä¢ File Storage  ‚îÇ
‚îÇ ‚Ä¢ Chart.js      ‚îÇ    ‚îÇ ‚Ä¢ Service Layer ‚îÇ    ‚îÇ ‚Ä¢ Caching       ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Error Handle  ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        Render Platform
```

#### **ÁõÆÊ®ô„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ôºà„Çπ„Ç±„Éº„É©„Éñ„É´Ë®≠Ë®àÔºâ**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Frontend Layer                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Progressive Web ‚îÇ   Admin Portal  ‚îÇ    Mobile App (Future)      ‚îÇ
‚îÇ App (PWA)       ‚îÇ   (Vue.js)      ‚îÇ    (React Native)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      API Gateway Layer                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Load Balancer   ‚îÇ Rate Limiting   ‚îÇ Authentication & CORS        ‚îÇ
‚îÇ (Nginx/Caddy)   ‚îÇ (Redis)         ‚îÇ (JWT + OAuth2)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Microservices Layer                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ User Service    ‚îÇLearning Service ‚îÇ    Analytics Service        ‚îÇ
‚îÇ (Python/Flask)  ‚îÇ(Python/FastAPI) ‚îÇ    (Python/FastAPI)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇQuestion Service ‚îÇContent Service  ‚îÇ    Notification Service     ‚îÇ
‚îÇ(Python/FastAPI) ‚îÇ(Python/Flask)   ‚îÇ    (Python/Celery)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       Data Layer                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PostgreSQL      ‚îÇ Redis Cache     ‚îÇ    Elasticsearch            ‚îÇ
‚îÇ (Primary DB)    ‚îÇ (Session/Cache) ‚îÇ    (Search/Analytics)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ MinIO/S3        ‚îÇ Message Queue   ‚îÇ    Time Series DB           ‚îÇ
‚îÇ (File Storage)  ‚îÇ (RabbitMQ)      ‚îÇ    (InfluxDB)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß „Éû„Ç§„ÇØ„É≠„Çµ„Éº„Éì„ÇπÂàÜËß£Êà¶Áï•

### „Çµ„Éº„Éì„ÇπÂ¢ÉÁïå„ÅÆÂÆöÁæ©

#### **1. User ServiceÔºàË™çË®º„ÉªË™çÂèØ„Éª„Éó„É≠„Éï„Ç°„Ç§„É´ÁÆ°ÁêÜÔºâ**

```python
# user_service/models.py
from sqlalchemy import Column, Integer, String, DateTime, Boolean
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False)
    username = Column(String(100), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    is_active = Column(Boolean, default=True)
    is_verified = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime)
    
    # OAuthÈÄ£Êê∫
    google_id = Column(String(255))
    github_id = Column(String(255))
    
class UserProfile(Base):
    __tablename__ = 'user_profiles'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    target_exam = Column(String(10), default='FE')
    target_date = Column(DateTime)
    daily_goal = Column(Integer, default=20)
    timezone = Column(String(50), default='Asia/Tokyo')
    learning_style = Column(String(20), default='balanced')  # visual, auditory, kinesthetic
    preferred_difficulty = Column(Integer, default=2)
    
# user_service/api.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.security import HTTPBearer
import jwt

app = FastAPI(title="User Service", version="1.0.0")
security = HTTPBearer()

@app.post("/auth/register")
async def register_user(user_data: UserRegistrationSchema):
    """„É¶„Éº„Ç∂„ÉºÁôªÈå≤"""
    # „Éë„Çπ„ÉØ„Éº„Éâ„Éè„ÉÉ„Ç∑„É•Âåñ
    password_hash = bcrypt.hashpw(user_data.password.encode(), bcrypt.gensalt())
    
    # „É¶„Éº„Ç∂„Éº‰ΩúÊàê
    user = User(
        email=user_data.email,
        username=user_data.username,
        password_hash=password_hash.decode()
    )
    
    db.add(user)
    db.commit()
    
    # Á¢∫Ë™ç„É°„Éº„É´ÈÄÅ‰ø°ÔºàÈùûÂêåÊúüÔºâ
    send_verification_email.delay(user.id, user.email)
    
    return {"message": "User registered successfully", "user_id": user.id}

@app.post("/auth/login")
async def login_user(credentials: LoginSchema):
    """„É≠„Ç∞„Ç§„É≥"""
    user = db.query(User).filter(User.email == credentials.email).first()
    
    if not user or not bcrypt.checkpw(credentials.password.encode(), user.password_hash.encode()):
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    # JWTÁîüÊàê
    token = jwt.encode({
        "user_id": user.id,
        "email": user.email,
        "exp": datetime.utcnow() + timedelta(hours=24)
    }, SECRET_KEY, algorithm="HS256")
    
    # „É≠„Ç∞„Ç§„É≥Â±•Ê≠¥Êõ¥Êñ∞
    user.last_login = datetime.utcnow()
    db.commit()
    
    return {
        "access_token": token,
        "token_type": "bearer",
        "expires_in": 86400
    }
```

#### **2. Learning ServiceÔºàÂ≠¶Áøí„Çª„ÉÉ„Ç∑„Éß„É≥„ÉªÈÄ≤ÊçóÁÆ°ÁêÜÔºâ**

```python
# learning_service/models.py
class LearningSession(Base):
    __tablename__ = 'learning_sessions'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, nullable=False)  # User ServiceÂèÇÁÖß
    session_type = Column(String(20))  # practice, test, review, challenge
    exam_type = Column(String(10), default='FE')
    category_filter = Column(JSON)  # ÂàÜÈáé„Éï„Ç£„É´„Çø„Éº
    difficulty_range = Column(JSON)  # Èõ£ÊòìÂ∫¶ÁØÑÂõ≤
    
    total_questions = Column(Integer)
    correct_answers = Column(Integer, default=0)
    
    start_time = Column(DateTime, default=datetime.utcnow)
    end_time = Column(DateTime)
    duration_seconds = Column(Integer)
    
    # AIÊé®Â•®„Éï„É©„Ç∞
    is_ai_recommended = Column(Boolean, default=False)
    recommendation_reason = Column(String(255))
    
class LearningRecord(Base):
    __tablename__ = 'learning_records'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(Integer, ForeignKey('learning_sessions.id'))
    user_id = Column(Integer, nullable=False)
    question_id = Column(Integer, nullable=False)  # Question ServiceÂèÇÁÖß
    
    user_answer = Column(Integer)
    is_correct = Column(Boolean)
    confidence_level = Column(Integer)  # 1-5„ÅÆËá™‰ø°Â∫¶
    response_time_ms = Column(Integer)
    
    # Â≠¶Áøí„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà
    attempt_number = Column(Integer, default=1)  # ÂêåÂïèÈ°å„ÅÆË©¶Ë°åÂõûÊï∞
    help_used = Column(Boolean, default=False)  # „Éí„É≥„Éà‰ΩøÁî®ÊúâÁÑ°
    bookmarked = Column(Boolean, default=False)  # „Éñ„ÉÉ„ÇØ„Éû„Éº„ÇØ
    
    created_at = Column(DateTime, default=datetime.utcnow)

# learning_service/ai_engine.py
class AdaptiveLearningEngine:
    def __init__(self):
        self.difficulty_model = self.load_difficulty_model()
        self.recommendation_model = self.load_recommendation_model()
    
    async def get_next_questions(self, user_id: int, session_config: dict) -> List[int]:
        """AI„Å´„Çà„ÇãÊ¨°„ÅÆÂïèÈ°åÊé®Â•®"""
        # „É¶„Éº„Ç∂„Éº„ÅÆÂ≠¶ÁøíÂ±•Ê≠¥ÂèñÂæó
        user_history = await self.get_user_learning_history(user_id)
        
        # ÁèæÂú®„ÅÆ„Çπ„Ç≠„É´„É¨„Éô„É´Êé®ÂÆö
        skill_level = self.estimate_skill_level(user_history)
        
        # Âº±ÁÇπÂàÜÈáéÁâπÂÆö
        weak_areas = self.identify_weak_areas(user_history)
        
        # ÊúÄÈÅ©ÂïèÈ°åÈÅ∏Âá∫
        if session_config.get('focus_on_weakness', True):
            questions = await self.select_weakness_focused_questions(
                weak_areas, skill_level, session_config
            )
        else:
            questions = await self.select_balanced_questions(
                skill_level, session_config
            )
        
        return questions
    
    def estimate_skill_level(self, history: List[LearningRecord]) -> Dict[str, float]:
        """ÂàÜÈáéÂà•„Çπ„Ç≠„É´„É¨„Éô„É´Êé®ÂÆöÔºàElo„É¨„Éº„ÉÜ„Ç£„É≥„Ç∞Ôºâ"""
        skill_levels = {}
        
        for category in self.get_all_categories():
            category_records = [r for r in history if r.question.category == category]
            
            if not category_records:
                skill_levels[category] = 1200  # „Éá„Éï„Ç©„É´„ÉàÂÄ§
                continue
            
            # Elo„É¨„Éº„ÉÜ„Ç£„É≥„Ç∞Ë®àÁÆó
            current_rating = 1200
            for record in sorted(category_records, key=lambda x: x.created_at):
                question_difficulty = record.question.difficulty_rating
                expected_score = 1 / (1 + 10**((question_difficulty - current_rating) / 400))
                actual_score = 1 if record.is_correct else 0
                
                # KÂõ†Â≠êÔºàÂ≠¶ÁøíÁéáÔºâ„ÅØË©¶Ë°åÂõûÊï∞„Å´Âøú„Åò„Å¶Ë™øÊï¥
                k_factor = max(32 - len(category_records) // 10, 16)
                current_rating += k_factor * (actual_score - expected_score)
            
            skill_levels[category] = current_rating
        
        return skill_levels
```

#### **3. Question ServiceÔºàÂïèÈ°å„Éá„Éº„ÇøÁÆ°ÁêÜ„ÉªÊ§úÁ¥¢Ôºâ**

```python
# question_service/models.py
class Question(Base):
    __tablename__ = 'questions'
    
    id = Column(Integer, primary_key=True)
    content_hash = Column(String(64), unique=True)  # ÈáçË§áÊ§úÂá∫Áî®
    
    # ÂïèÈ°åÂÜÖÂÆπ
    question_text = Column(Text, nullable=False)
    choices = Column(JSON, nullable=False)  # ÈÅ∏ÊäûËÇ¢ÈÖçÂàó
    correct_answer = Column(Integer, nullable=False)
    explanation = Column(Text)
    
    # ÂàÜÈ°ûÊÉÖÂ†±
    exam_type = Column(String(10), nullable=False)  # FE, AP, IP, SG
    category = Column(String(100), nullable=False)
    subcategory = Column(String(100))
    keywords = Column(JSON)  # Ê§úÁ¥¢Áî®„Ç≠„Éº„ÉØ„Éº„Éâ
    
    # Èõ£ÊòìÂ∫¶„ÉªÈáçË¶ÅÂ∫¶
    difficulty_level = Column(Integer, default=2)  # 1-5
    importance_score = Column(Float, default=1.0)  # Âá∫È°åÈ†ªÂ∫¶„Éô„Éº„Çπ
    difficulty_rating = Column(Float, default=1200)  # Elo„É¨„Éº„ÉÜ„Ç£„É≥„Ç∞
    
    # „É°„Çø„Éá„Éº„Çø
    year = Column(Integer)
    source = Column(String(100))  # Âá∫ÂÖ∏
    version = Column(Integer, default=1)
    
    # Èñ¢ÈÄ£ÊÄß
    related_questions = Column(JSON)  # Èñ¢ÈÄ£ÂïèÈ°åIDÈÖçÂàó
    prerequisite_concepts = Column(JSON)  # ÂâçÊèêÊ¶ÇÂøµ
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow)

# question_service/search.py
from elasticsearch import Elasticsearch

class QuestionSearchEngine:
    def __init__(self):
        self.es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])
        self.setup_indices()
    
    def setup_indices(self):
        """Elasticsearch„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπË®≠ÂÆö"""
        mapping = {
            "mappings": {
                "properties": {
                    "question_text": {
                        "type": "text",
                        "analyzer": "kuromoji"  # Êó•Êú¨Ë™ûËß£Êûê
                    },
                    "explanation": {
                        "type": "text", 
                        "analyzer": "kuromoji"
                    },
                    "category": {"type": "keyword"},
                    "exam_type": {"type": "keyword"},
                    "difficulty_level": {"type": "integer"},
                    "keywords": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        self.es.indices.create(index="questions", body=mapping, ignore=400)
    
    async def search_questions(self, query: QuestionSearchQuery) -> List[Dict]:
        """È´òÂ∫¶„Å™ÂïèÈ°åÊ§úÁ¥¢"""
        search_body = {
            "query": {
                "bool": {
                    "must": [],
                    "filter": [],
                    "should": []
                }
            },
            "sort": [],
            "highlight": {
                "fields": {
                    "question_text": {},
                    "explanation": {}
                }
            }
        }
        
        # „ÉÜ„Ç≠„Çπ„ÉàÊ§úÁ¥¢
        if query.text:
            search_body["query"]["bool"]["must"].append({
                "multi_match": {
                    "query": query.text,
                    "fields": ["question_text^2", "explanation", "keywords"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            })
        
        # „Éï„Ç£„É´„Çø„ÉºÊù°‰ª∂
        if query.exam_type:
            search_body["query"]["bool"]["filter"].append({
                "term": {"exam_type": query.exam_type}
            })
        
        if query.categories:
            search_body["query"]["bool"]["filter"].append({
                "terms": {"category": query.categories}
            })
        
        if query.difficulty_range:
            search_body["query"]["bool"]["filter"].append({
                "range": {
                    "difficulty_level": {
                        "gte": query.difficulty_range.min,
                        "lte": query.difficulty_range.max
                    }
                }
            })
        
        # „ÇΩ„Éº„Éà
        if query.sort_by == "relevance":
            search_body["sort"] = ["_score"]
        elif query.sort_by == "difficulty":
            search_body["sort"] = [{"difficulty_level": {"order": "asc"}}]
        elif query.sort_by == "newest":
            search_body["sort"] = [{"created_at": {"order": "desc"}}]
        
        response = self.es.search(
            index="questions",
            body=search_body,
            size=query.limit,
            from_=query.offset
        )
        
        return self.format_search_results(response)
```

#### **4. Analytics ServiceÔºàÂ≠¶ÁøíÂàÜÊûê„Éª„É¨„Éù„Éº„ÉàÁîüÊàêÔºâ**

```python
# analytics_service/models.py
class LearningAnalytics(Base):
    __tablename__ = 'learning_analytics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, nullable=False)
    date = Column(Date, nullable=False)
    
    # Êó•Ê¨°Áµ±Ë®à
    total_questions = Column(Integer, default=0)
    correct_answers = Column(Integer, default=0)
    study_time_minutes = Column(Integer, default=0)
    sessions_count = Column(Integer, default=0)
    
    # ÂàÜÈáéÂà•Ë©≥Á¥∞ÔºàJSONÔºâ
    category_performance = Column(JSON)
    difficulty_distribution = Column(JSON)
    response_time_stats = Column(JSON)
    
    # Â≠¶ÁøíÂìÅË≥™ÊåáÊ®ô
    focus_score = Column(Float)  # ÈõÜ‰∏≠Â∫¶Ôºà„Çª„ÉÉ„Ç∑„Éß„É≥ÊôÇÈñì/Á∑èÊôÇÈñìÔºâ
    consistency_score = Column(Float)  # ‰∏ÄË≤´ÊÄßÔºàÊ≠£Á≠îÁéá„ÅÆÂÆâÂÆöÊÄßÔºâ
    improvement_rate = Column(Float)  # ÊîπÂñÑÁéáÔºàÂâçÊó•ÊØîÔºâ
    
    created_at = Column(DateTime, default=datetime.utcnow)

# analytics_service/reports.py
class ReportGenerator:
    def __init__(self):
        self.db = get_database_connection()
        self.ml_engine = MachineLearningEngine()
    
    async def generate_weekly_report(self, user_id: int) -> Dict:
        """ÈÄ±Ê¨°Â≠¶Áøí„É¨„Éù„Éº„ÉàÁîüÊàê"""
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=7)
        
        # Âü∫Êú¨Áµ±Ë®àÂèñÂæó
        analytics = await self.get_analytics_data(user_id, start_date, end_date)
        
        # ÂâçÈÄ±ÊØîËºÉ
        prev_week_analytics = await self.get_analytics_data(
            user_id, start_date - timedelta(days=7), start_date
        )
        
        # Â≠¶Áøí„Éë„Çø„Éº„É≥ÂàÜÊûê
        learning_patterns = await self.analyze_learning_patterns(user_id, analytics)
        
        # AI „Å´„Çà„ÇãÊîπÂñÑÊèêÊ°à
        recommendations = await self.ml_engine.generate_recommendations(user_id, analytics)
        
        # Âº±ÁÇπ„ÉªÂº∑„ÅøÂàÜÊûê
        strengths_weaknesses = await self.analyze_strengths_weaknesses(user_id, analytics)
        
        return {
            "period": {"start": start_date, "end": end_date},
            "summary": {
                "total_questions": sum(a.total_questions for a in analytics),
                "accuracy_rate": self.calculate_accuracy_rate(analytics),
                "study_time": sum(a.study_time_minutes for a in analytics),
                "improvement": self.calculate_improvement(analytics, prev_week_analytics)
            },
            "daily_breakdown": self.format_daily_breakdown(analytics),
            "category_performance": self.analyze_category_performance(analytics),
            "learning_patterns": learning_patterns,
            "recommendations": recommendations,
            "strengths_weaknesses": strengths_weaknesses,
            "goals": await self.check_goal_progress(user_id, analytics)
        }
    
    async def analyze_learning_patterns(self, user_id: int, analytics: List) -> Dict:
        """Â≠¶Áøí„Éë„Çø„Éº„É≥„ÅÆÂàÜÊûê"""
        patterns = {
            "peak_hours": [],
            "optimal_session_length": 0,
            "consistency_score": 0,
            "learning_velocity": 0
        }
        
        # ÊôÇÈñìÂ∏ØÂà•„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê
        hourly_performance = {}
        sessions = await self.get_user_sessions(user_id, days=30)
        
        for session in sessions:
            hour = session.start_time.hour
            if hour not in hourly_performance:
                hourly_performance[hour] = []
            
            accuracy = session.correct_answers / session.total_questions if session.total_questions > 0 else 0
            hourly_performance[hour].append(accuracy)
        
        # ÊúÄÈÅ©ÊôÇÈñìÂ∏ØÁâπÂÆö
        peak_performance = {}
        for hour, performances in hourly_performance.items():
            if len(performances) >= 3:  # ÊúÄ‰Ωé3„Çª„ÉÉ„Ç∑„Éß„É≥
                peak_performance[hour] = np.mean(performances)
        
        if peak_performance:
            best_hours = sorted(peak_performance.items(), key=lambda x: x[1], reverse=True)[:3]
            patterns["peak_hours"] = [{"hour": h, "accuracy": acc} for h, acc in best_hours]
        
        # ÊúÄÈÅ©„Çª„ÉÉ„Ç∑„Éß„É≥ÊôÇÈñìÂàÜÊûê
        session_lengths = [(s.duration_seconds // 60, s.correct_answers / s.total_questions) 
                          for s in sessions if s.duration_seconds and s.total_questions > 0]
        
        if session_lengths:
            # „Çª„ÉÉ„Ç∑„Éß„É≥ÊôÇÈñì„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÁõ∏Èñ¢
            duration_groups = {}
            for duration, accuracy in session_lengths:
                group = (duration // 15) * 15  # 15ÂàÜÂçò‰Ωç„Åß„Ç∞„É´„Éº„ÉóÂåñ
                if group not in duration_groups:
                    duration_groups[group] = []
                duration_groups[group].append(accuracy)
            
            best_duration = max(duration_groups.items(), 
                              key=lambda x: np.mean(x[1]) if len(x[1]) >= 2 else 0)
            patterns["optimal_session_length"] = best_duration[0]
        
        return patterns
```

---

## üöÄ ÈñãÁô∫„ÉªÈÅãÁî®ÊîπÂñÑÊèêÊ°à

### DevOps Pipeline Âº∑Âåñ

#### **CI/CD PipelineÔºàGitHub ActionsÔºâ**

```yaml
# .github/workflows/comprehensive-pipeline.yml
name: Comprehensive CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: it-exam-learning-system

jobs:
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Lint with flake8
        run: |
          flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      
      - name: Format check with black
        run: black --check src/
      
      - name: Type check with mypy
        run: mypy src/ --ignore-missing-imports
      
      - name: Security check with bandit
        run: bandit -r src/ -f json -o bandit-report.json
      
      - name: Upload security report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json

  unit-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run unit tests
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 30  # „Çµ„Éº„Éì„ÇπËµ∑ÂãïÂæÖ„Å°
      
      - name: Run integration tests
        run: |
          pytest tests/integration/ -v --maxfail=5
      
      - name: Cleanup
        if: always()
        run: docker-compose -f docker-compose.test.yml down

  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install Playwright
        run: |
          npm install -g @playwright/test
          playwright install
      
      - name: Start application
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 60  # „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ëµ∑ÂãïÂæÖ„Å°
      
      - name: Run E2E tests
        run: |
          playwright test tests/e2e/
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: playwright-report/

  security-scan:
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

  build-and-push:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile.prod
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
      - name: Deploy to staging
        run: |
          curl -X POST "${{ secrets.STAGING_DEPLOY_WEBHOOK }}" \
            -H "Content-Type: application/json" \
            -d '{"ref": "${{ github.sha }}"}'

  deploy-production:
    runs-on: ubuntu-latest
    needs: [build-and-push, e2e-tests]
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Deploy to production
        run: |
          curl -X POST "${{ secrets.PRODUCTION_DEPLOY_WEBHOOK }}" \
            -H "Content-Type: application/json" \
            -d '{"ref": "${{ github.sha }}"}'
      
      - name: Run smoke tests
        run: |
          sleep 120  # „Éá„Éó„É≠„Ç§ÂÆå‰∫ÜÂæÖ„Å°
          curl -f https://it-exam-learning-system.com/health || exit 1
          
      - name: Notify deployment
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### Infrastructure as Code

#### **Terraform Configuration**

```hcl
# infrastructure/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "it-exam-terraform-state"
    key    = "prod/terraform.tfstate"
    region = "ap-northeast-1"
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC Configuration
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 3.0"
  
  name = "it-exam-vpc"
  cidr = "10.0.0.0/16"
  
  azs             = ["ap-northeast-1a", "ap-northeast-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]
  
  enable_nat_gateway = true
  enable_vpn_gateway = false
  
  tags = local.common_tags
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "it-exam-cluster"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
  
  tags = local.common_tags
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "it-exam-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets           = module.vpc.public_subnets
  
  enable_deletion_protection = false
  
  tags = local.common_tags
}

# RDS PostgreSQL
resource "aws_db_instance" "main" {
  identifier = "it-exam-db"
  
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = "db.t3.micro"
  
  allocated_storage     = 20
  max_allocated_storage = 100
  storage_type         = "gp2"
  storage_encrypted    = true
  
  db_name  = "learning_system"
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = true
  deletion_protection = false
  
  performance_insights_enabled = true
  monitoring_interval         = 60
  monitoring_role_arn        = aws_iam_role.rds_monitoring.arn
  
  tags = local.common_tags
}

# ElastiCache Redis
resource "aws_elasticache_subnet_group" "main" {
  name       = "it-exam-cache-subnet"
  subnet_ids = module.vpc.private_subnets
}

resource "aws_elasticache_replication_group" "main" {
  replication_group_id         = "it-exam-redis"
  description                  = "Redis cluster for IT Exam Learning System"
  
  port               = 6379
  parameter_group_name = "default.redis7"
  node_type          = "cache.t3.micro"
  num_cache_clusters = 2
  
  subnet_group_name  = aws_elasticache_subnet_group.main.name
  security_group_ids = [aws_security_group.redis.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  
  tags = local.common_tags
}

# CloudWatch Log Groups
resource "aws_cloudwatch_log_group" "app" {
  name              = "/ecs/it-exam-app"
  retention_in_days = 30
  
  tags = local.common_tags
}

# Local values
locals {
  common_tags = {
    Project     = "IT Exam Learning System"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}
```

### Monitoring & Observability

#### **Prometheus + Grafana Ë®≠ÂÆö**

```yaml
# monitoring/docker-compose.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:
```

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
  
  - job_name: 'it-exam-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
  
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
```

#### **„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„É°„Éà„É™„ÇØ„Çπ**

```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from flask import Response
import time
import psutil

# „Ç´„Çπ„Çø„É†„É°„Éà„É™„ÇØ„ÇπÂÆöÁæ©
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_USERS = Gauge(
    'active_users_total',
    'Number of active users'
)

LEARNING_SESSIONS = Counter(
    'learning_sessions_total',
    'Total learning sessions',
    ['exam_type', 'session_type']
)

QUESTION_RESPONSES = Counter(
    'question_responses_total',
    'Total question responses',
    ['category', 'is_correct']
)

DATABASE_CONNECTIONS = Gauge(
    'database_connections_active',
    'Active database connections'
)

SYSTEM_MEMORY_USAGE = Gauge(
    'system_memory_usage_bytes',
    'System memory usage in bytes'
)

class MetricsCollector:
    def __init__(self, app):
        self.app = app
        self.setup_middleware()
        self.setup_metrics_endpoint()
    
    def setup_middleware(self):
        @self.app.before_request
        def before_request():
            request.start_time = time.time()
        
        @self.app.after_request
        def after_request(response):
            # „É™„ÇØ„Ç®„Çπ„ÉàÊôÇÈñìË®òÈå≤
            duration = time.time() - request.start_time
            REQUEST_DURATION.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown'
            ).observe(duration)
            
            # „É™„ÇØ„Ç®„Çπ„ÉàÊï∞Ë®òÈå≤
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown',
                status=response.status_code
            ).inc()
            
            return response
    
    def setup_metrics_endpoint(self):
        @self.app.route('/metrics')
        def metrics():
            # „Ç∑„Çπ„ÉÜ„É†„É°„Éà„É™„ÇØ„ÇπÊõ¥Êñ∞
            self.update_system_metrics()
            
            # Prometheus„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßÂá∫Âäõ
            return Response(
                generate_latest(),
                mimetype='text/plain'
            )
    
    def update_system_metrics(self):
        # „É°„É¢„É™‰ΩøÁî®Èáè
        memory = psutil.virtual_memory()
        SYSTEM_MEMORY_USAGE.set(memory.used)
        
        # „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„É¶„Éº„Ç∂„ÉºÊï∞ÔºàRedis „Åã„ÇâÂèñÂæóÔºâ
        active_users = self.get_active_users_count()
        ACTIVE_USERS.set(active_users)
        
        # „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂öÊï∞
        db_connections = self.get_db_connections_count()
        DATABASE_CONNECTIONS.set(db_connections)
    
    def record_learning_activity(self, exam_type: str, session_type: str):
        """Â≠¶ÁøíÊ¥ªÂãï„ÅÆË®òÈå≤"""
        LEARNING_SESSIONS.labels(
            exam_type=exam_type,
            session_type=session_type
        ).inc()
    
    def record_question_response(self, category: str, is_correct: bool):
        """ÂïèÈ°åÂõûÁ≠î„ÅÆË®òÈå≤"""
        QUESTION_RESPONSES.labels(
            category=category,
            is_correct=str(is_correct).lower()
        ).inc()
```

---

## üìä ÂÆüË£Ö„É≠„Éº„Éâ„Éû„ÉÉ„Éó

### Phase 1: Âü∫Áõ§Âº∑ÂåñÔºà4-6ÈÄ±ÈñìÔºâ

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u73fe\u72b6\u5206\u6790\u3068\u30e6\u30fc\u30b6\u30fc\u30cb\u30fc\u30ba\u8abf\u67fb", "status": "completed", "priority": "high", "id": "1"}, {"content": "\u6a5f\u80fd\u8981\u4ef6\u306e\u8a73\u7d30\u5b9a\u7fa9", "status": "completed", "priority": "high", "id": "2"}, {"content": "\u6280\u8853\u8981\u4ef6\u3068\u5236\u7d04\u4e8b\u9805\u306e\u6574\u7406", "status": "completed", "priority": "high", "id": "3"}, {"content": "UI/UX\u6539\u5584\u63d0\u6848", "status": "completed", "priority": "medium", "id": "4"}, {"content": "\u30b7\u30b9\u30c6\u30e0\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u5f37\u5316\u6848", "status": "completed", "priority": "medium", "id": "5"}, {"content": "\u958b\u767a\u30fb\u904b\u7528\u6539\u5584\u63d0\u6848", "status": "in_progress", "priority": "medium", "id": "6"}, {"content": "\u5b9f\u88c5\u512a\u5148\u5ea6\u3068\u30ed\u30fc\u30c9\u30de\u30c3\u30d7\u4f5c\u6210", "status": "pending", "priority": "high", "id": "7"}]