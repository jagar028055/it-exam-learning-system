# IT試験学習システム - 技術要件・制約事項定義書

**策定日**: 2025年7月30日
**プロジェクト**: 情報技術者試験学習システム 包括的改善
**対象**: 技術要件・制約事項詳細定義

---

## 🏗️ システムアーキテクチャ要件

### 現状アーキテクチャの評価と改善方針

**現状**:
```
Frontend: Jinja2 Templates + Bootstrap + JavaScript
Backend: Flask + Blueprint Pattern + Service Layer
Database: SQLite + Optimized Queries + Caching
Deployment: Render + Gunicorn + GitHub Actions
```

**評価**:
✅ **優秀な点**: 
- 適切なレイヤー分離
- サービス指向アーキテクチャ
- テンプレートコンポーネント化

⚠️ **改善点**:
- スケーラビリティの限界
- リアルタイム機能の欠如
- 高度なデータ分析機能の不足

### 🎯 目標アーキテクチャ

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[Progressive Web App]
        WS[WebSocket Client]
        SW[Service Worker]
    end
    
    subgraph "API Gateway"
        GW[Rate Limiting + Auth]
        CACHE[Redis Cache]
    end
    
    subgraph "Application Layer"
        API[REST API Service]
        RT[Real-time Service]
        ML[ML Analysis Service]
        BG[Background Jobs]
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL)]
        ES[(Elasticsearch)]
        S3[(File Storage)]
        MQ[Message Queue]
    end
    
    UI --> GW
    WS --> RT
    GW --> API
    API --> PG
    ML --> ES
    BG --> MQ
```

---

## 💾 データベース要件

### データベース移行戦略

#### **Phase 1: SQLite拡張（即座実行可能）**

```sql
-- 既存テーブルの最適化
CREATE INDEX idx_questions_category ON questions(category, difficulty);
CREATE INDEX idx_learning_records_session ON learning_records(session_id, attempt_date);
CREATE INDEX idx_study_sessions_user_date ON study_sessions(user_id, start_time);

-- 新機能対応テーブル
CREATE TABLE user_profiles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT UNIQUE NOT NULL,
    target_exam TEXT DEFAULT 'FE', 
    target_date DATE,
    daily_goal INTEGER DEFAULT 20,
    preferred_difficulty INTEGER DEFAULT 2,
    learning_style TEXT DEFAULT 'balanced',
    timezone TEXT DEFAULT 'Asia/Tokyo',
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE learning_insights (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    insight_type TEXT NOT NULL, -- 'weakness', 'strength', 'recommendation'
    content TEXT NOT NULL,
    confidence_score REAL DEFAULT 0.5,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    expires_at DATETIME
);

CREATE TABLE gamification_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    points INTEGER DEFAULT 0,
    level INTEGER DEFAULT 1,
    streak_days INTEGER DEFAULT 0,
    achievements TEXT, -- JSON配列
    last_activity DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### **Phase 2: PostgreSQL移行（スケール時）**

```sql
-- 本格運用時のPostgreSQL設計
CREATE TABLE questions (
    id SERIAL PRIMARY KEY,
    question_text TEXT NOT NULL,
    choices JSONB NOT NULL,
    correct_answer INTEGER NOT NULL,
    category VARCHAR(100) NOT NULL,
    subcategory VARCHAR(100),
    difficulty_level INTEGER CHECK (difficulty_level BETWEEN 1 AND 5),
    importance_score REAL DEFAULT 1.0,
    year INTEGER,
    exam_type VARCHAR(10) DEFAULT 'FE',
    explanation TEXT,
    keywords TEXT[], -- PostgreSQL配列型
    related_questions INTEGER[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 高性能インデックス
CREATE INDEX CONCURRENTLY idx_questions_full_text ON questions 
USING GIN (to_tsvector('japanese', question_text || ' ' || explanation));

CREATE INDEX CONCURRENTLY idx_questions_composite ON questions 
(exam_type, category, difficulty_level) 
INCLUDE (id, importance_score);
```

### 📊 分析データベース設計

```sql
-- 学習分析用マテリアライズドビュー
CREATE MATERIALIZED VIEW learning_analytics AS
SELECT 
    DATE_TRUNC('day', lr.attempt_date) as learning_date,
    lr.question_id,
    q.category,
    q.difficulty_level,
    COUNT(*) as attempt_count,
    AVG(CASE WHEN lr.is_correct THEN 1.0 ELSE 0.0 END) as accuracy_rate,
    AVG(lr.response_time) as avg_response_time,
    COUNT(DISTINCT lr.session_id) as session_count
FROM learning_records lr
JOIN questions q ON lr.question_id = q.id
GROUP BY 1, 2, 3, 4;

-- 定期更新ジョブ
CREATE OR REPLACE FUNCTION refresh_learning_analytics()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY learning_analytics;
END;
$$ LANGUAGE plpgsql;
```

---

## 🚀 パフォーマンス要件

### キャッシング戦略

#### **Multi-layer Caching**

```python
# L1: Application Cache (Flask-Caching)
from flask_caching import Cache

cache_config = {
    'CACHE_TYPE': 'RedisCache',
    'CACHE_REDIS_URL': 'redis://localhost:6379/0',
    'CACHE_DEFAULT_TIMEOUT': 300
}

# L2: Database Query Cache
class OptimizedQuestionService:
    @cache.memoize(timeout=3600)
    def get_questions_by_category(self, category: str, limit: int = 50):
        """カテゴリ別問題取得（1時間キャッシュ）"""
        pass
    
    @cache.memoize(timeout=300) 
    def get_user_recommendations(self, user_id: str):
        """ユーザー推奨問題（5分キャッシュ）"""
        pass

# L3: CDN Cache (Static Assets)
# nginx.conf
location /static/ {
    expires 1y;
    add_header Cache-Control "public, immutable";
}
```

#### **Database Connection Pooling**

```python
# SQLAlchemy Connection Pool最適化
from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool

engine = create_engine(
    'postgresql://user:pass@host/db',
    poolclass=StaticPool,
    pool_size=20,
    max_overflow=30,
    pool_timeout=30,
    pool_recycle=3600,
    echo=False
)
```

### 📈 スケーラビリティ対応

#### **Horizontal Scaling Strategy**

```yaml
# Docker Compose for Multi-Service
version: '3.8'
services:
  web:
    build: .
    ports:
      - "8000-8003:8000"
    replicas: 4
    environment:
      - DATABASE_URL=postgresql://...
      - REDIS_URL=redis://redis:6379
      
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - web
      
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
      
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: learning_system
    volumes:
      - postgres_data:/var/lib/postgresql/data
```

---

## 🤖 機械学習要件

### 学習分析エンジン

#### **Recommendation Algorithm**

```python
import numpy as np
import pandas as pd
from sklearn.collaborative_filtering import NMF
from sklearn.metrics.pairwise import cosine_similarity

class IntelligentRecommendationEngine:
    def __init__(self):
        self.model = NMF(n_components=50, random_state=42)
        self.user_item_matrix = None
        
    def train_model(self, learning_records: pd.DataFrame):
        """協調フィルタリングモデルの訓練"""
        # ユーザー-問題マトリックス作成
        self.user_item_matrix = learning_records.pivot_table(
            index='user_id', 
            columns='question_id', 
            values='is_correct',
            fill_value=0
        )
        
        # 非負値行列分解
        self.model.fit(self.user_item_matrix)
        
    def recommend_questions(self, user_id: str, n_recommendations: int = 10):
        """個人化された問題推奨"""
        if user_id not in self.user_item_matrix.index:
            return self._cold_start_recommendations(n_recommendations)
            
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        user_factors = self.model.transform(self.user_item_matrix.iloc[user_idx:user_idx+1])
        
        # 推奨スコア計算
        item_factors = self.model.components_
        scores = np.dot(user_factors, item_factors)[0]
        
        # 未回答問題から推奨
        answered_questions = self.user_item_matrix.iloc[user_idx].nonzero()[0]
        unanswered_mask = np.ones(len(scores), dtype=bool)
        unanswered_mask[answered_questions] = False
        
        top_indices = np.argsort(scores[unanswered_mask])[-n_recommendations:]
        return self.user_item_matrix.columns[unanswered_mask][top_indices]
```

#### **Adaptive Difficulty System**

```python
class AdaptiveDifficultyEngine:
    def __init__(self):
        self.elo_k_factor = 32
        self.default_rating = 1200
        
    def calculate_question_difficulty(self, question_id: int) -> float:
        """問題の難易度をEloレーティングで計算"""
        records = self.get_question_records(question_id)
        
        if not records:
            return self.default_rating
            
        # Eloレーティングシステム適用
        total_rating = 0
        total_weight = 0
        
        for record in records:
            user_skill = self.get_user_skill_level(record.user_id)
            expected_score = 1 / (1 + 10**((self.default_rating - user_skill) / 400))
            actual_score = 1 if record.is_correct else 0
            
            # 重み付け（最近の記録ほど重要）
            days_ago = (datetime.now() - record.attempt_date).days
            weight = np.exp(-days_ago / 30)  # 30日で半減
            
            total_rating += weight * (self.default_rating + self.elo_k_factor * (actual_score - expected_score))
            total_weight += weight
            
        return total_rating / total_weight if total_weight > 0 else self.default_rating
```

---

## 🔒 セキュリティ要件

### 認証・認可システム

#### **Multi-Factor Authentication**

```python
from flask_jwt_extended import JWTManager
import pyotp
import qrcode

class SecureAuthSystem:
    def __init__(self, app):
        self.jwt = JWTManager(app)
        
    def generate_2fa_secret(self, user_id: str) -> str:
        """2FA用秘密鍵生成"""
        secret = pyotp.random_base32()
        
        # QRコード生成
        totp_uri = pyotp.totp.TOTP(secret).provisioning_uri(
            name=user_id,
            issuer_name="IT Exam Learning System"
        )
        
        qr = qrcode.QRCode(version=1, box_size=10, border=5)
        qr.add_data(totp_uri)
        qr.make(fit=True)
        
        return secret, qr.make_image(fill_color="black", back_color="white")
    
    def verify_2fa_token(self, secret: str, token: str) -> bool:
        """2FAトークン検証"""
        totp = pyotp.TOTP(secret)
        return totp.verify(token, valid_window=1)
```

#### **Data Protection**

```python
from cryptography.fernet import Fernet
import hashlib

class DataProtectionService:
    def __init__(self, encryption_key: bytes):
        self.cipher = Fernet(encryption_key)
        
    def encrypt_personal_data(self, data: str) -> str:
        """個人情報の暗号化"""
        return self.cipher.encrypt(data.encode()).decode()
    
    def decrypt_personal_data(self, encrypted_data: str) -> str:
        """個人情報の復号化"""
        return self.cipher.decrypt(encrypted_data.encode()).decode()
    
    def hash_sensitive_data(self, data: str) -> str:
        """機密データのハッシュ化"""
        return hashlib.sha256(data.encode()).hexdigest()
```

---

## 📊 監視・ログ要件

### アプリケーション監視

```python
from prometheus_flask_exporter import PrometheusMetrics
import structlog

# メトリクス収集
metrics = PrometheusMetrics(app)

# カスタムメトリクス
learning_session_counter = Counter(
    'learning_sessions_total',
    'Total number of learning sessions',
    ['exam_type', 'study_mode']
)

question_response_time = Histogram(
    'question_response_seconds',
    'Time spent answering questions',
    ['category', 'difficulty']
)

# 構造化ログ
logger = structlog.get_logger()

@app.route('/api/questions/<int:question_id>/answer', methods=['POST'])
def submit_answer(question_id):
    start_time = time.time()
    
    try:
        # ビジネスロジック
        result = process_answer(question_id, request.json)
        
        # メトリクス記録
        response_time = time.time() - start_time
        question_response_time.labels(
            category=result.category,
            difficulty=result.difficulty
        ).observe(response_time)
        
        # ログ記録
        logger.info(
            "answer_submitted",
            question_id=question_id,
            user_id=get_current_user_id(),
            is_correct=result.is_correct,
            response_time=response_time
        )
        
        return jsonify(result.to_dict())
        
    except Exception as e:
        logger.error(
            "answer_submission_failed",
            question_id=question_id,
            error=str(e),
            traceback=traceback.format_exc()
        )
        raise
```

---

## 🔧 開発・運用要件

### CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production
on:
  push:
    branches: [main]
    
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      - name: Run tests
        run: |
          pytest --cov=src --cov-report=xml
          flake8 src/
          black --check src/
      - name: Security scan
        run: bandit -r src/
        
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Render
        run: |
          curl -X POST ${{ secrets.RENDER_DEPLOY_HOOK }}
```

### Infrastructure as Code

```yaml
# docker-compose.production.yml
version: '3.8'
services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.prod
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - SECRET_KEY=${SECRET_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.prod.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - web
      
  redis:
    image: redis:alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
      
volumes:
  redis_data:
```

---

## 🎯 制約事項・リスク

### 技術的制約
- **現在のRender環境**: メモリ512MB、スケーリング制限
- **SQLite制限**: 同時書き込み制限、ファイルサイズ上限
- **フロントエンド**: SPA化せずサーバーサイドレンダリング維持

### 運用制約
- **予算制限**: 月額$50以下のインフラ費用
- **運用体制**: 基本的に自動化、最小限の手動運用
- **データ移行**: ダウンタイム最小化

### リスク評価・対策

| リスク | 確率 | 影響 | 対策 |
|--------|------|------|------|
| スケーラビリティ不足 | 中 | 高 | PostgreSQL移行計画 |
| セキュリティ脆弱性 | 低 | 高 | 定期セキュリティ監査 |
| データ破損 | 低 | 高 | 自動バックアップ + テスト |
| パフォーマンス劣化 | 中 | 中 | 監視アラート + 最適化 |

---

## 📋 実装マイルストーン

### Phase 1: 基盤強化（2-3週間）
- [ ] データベース拡張・最適化
- [ ] キャッシングシステム導入
- [ ] セキュリティ強化
- [ ] 監視システム構築

### Phase 2: 高度機能（4-6週間）
- [ ] ML推奨システム実装
- [ ] リアルタイム機能追加
- [ ] 高度分析機能
- [ ] パフォーマンス最適化

### Phase 3: スケーリング（必要時）
- [ ] PostgreSQL移行
- [ ] マイクロサービス化
- [ ] Kubernetes対応
- [ ] CDN・負荷分散

---

この技術要件に基づき、現在のシステムを段階的に進化させ、
高性能・高可用性・高セキュリティな学習プラットフォームを構築します。