# IT試験学習システム - システム設計・開発運用改善提案書

**策定日**: 2025年7月30日
**プロジェクト**: 情報技術者試験学習システム 包括的改善
**対象**: システム設計・開発・運用プロセス改善

---

## 🏗️ 次世代システムアーキテクチャ

### Current vs Target Architecture

#### **現在のアーキテクチャ（改善済み）**
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend      │    │   Backend       │    │   Database      │
│                 │    │                 │    │                 │
│ • Jinja2        │───▶│ • Flask         │───▶│ • SQLite        │
│ • Bootstrap     │    │ • Blueprint     │    │ • File Storage  │
│ • Chart.js      │    │ • Service Layer │    │ • Caching       │
│                 │    │ • Error Handle  │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────▼───────────────────────┘
                        Render Platform
```

#### **目標アーキテクチャ（スケーラブル設計）**
```
┌─────────────────────────────────────────────────────────────────┐
│                        Frontend Layer                           │
├─────────────────┬─────────────────┬─────────────────────────────┤
│ Progressive Web │   Admin Portal  │    Mobile App (Future)      │
│ App (PWA)       │   (Vue.js)      │    (React Native)           │
└─────────────────┴─────────────────┴─────────────────────────────┘
         │                       │                       │
         └───────────────────────▼───────────────────────┘
┌─────────────────────────────────────────────────────────────────┐
│                      API Gateway Layer                         │
├─────────────────┬─────────────────┬─────────────────────────────┤
│ Load Balancer   │ Rate Limiting   │ Authentication & CORS        │
│ (Nginx/Caddy)   │ (Redis)         │ (JWT + OAuth2)              │
└─────────────────┴─────────────────┴─────────────────────────────┘
         │                       │                       │
         └───────────────────────▼───────────────────────┘
┌─────────────────────────────────────────────────────────────────┐
│                    Microservices Layer                         │
├─────────────────┬─────────────────┬─────────────────────────────┤
│ User Service    │Learning Service │    Analytics Service        │
│ (Python/Flask)  │(Python/FastAPI) │    (Python/FastAPI)         │
├─────────────────┼─────────────────┼─────────────────────────────┤
│Question Service │Content Service  │    Notification Service     │
│(Python/FastAPI) │(Python/Flask)   │    (Python/Celery)          │
└─────────────────┴─────────────────┴─────────────────────────────┘
         │                       │                       │
         └───────────────────────▼───────────────────────┘
┌─────────────────────────────────────────────────────────────────┐
│                       Data Layer                               │
├─────────────────┬─────────────────┬─────────────────────────────┤
│ PostgreSQL      │ Redis Cache     │    Elasticsearch            │
│ (Primary DB)    │ (Session/Cache) │    (Search/Analytics)       │
├─────────────────┼─────────────────┼─────────────────────────────┤
│ MinIO/S3        │ Message Queue   │    Time Series DB           │
│ (File Storage)  │ (RabbitMQ)      │    (InfluxDB)               │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

---

## 🔧 マイクロサービス分解戦略

### サービス境界の定義

#### **1. User Service（認証・認可・プロファイル管理）**

```python
# user_service/models.py
from sqlalchemy import Column, Integer, String, DateTime, Boolean
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    email = Column(String(255), unique=True, nullable=False)
    username = Column(String(100), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    is_active = Column(Boolean, default=True)
    is_verified = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime)
    
    # OAuth連携
    google_id = Column(String(255))
    github_id = Column(String(255))
    
class UserProfile(Base):
    __tablename__ = 'user_profiles'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    target_exam = Column(String(10), default='FE')
    target_date = Column(DateTime)
    daily_goal = Column(Integer, default=20)
    timezone = Column(String(50), default='Asia/Tokyo')
    learning_style = Column(String(20), default='balanced')  # visual, auditory, kinesthetic
    preferred_difficulty = Column(Integer, default=2)
    
# user_service/api.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.security import HTTPBearer
import jwt

app = FastAPI(title="User Service", version="1.0.0")
security = HTTPBearer()

@app.post("/auth/register")
async def register_user(user_data: UserRegistrationSchema):
    """ユーザー登録"""
    # パスワードハッシュ化
    password_hash = bcrypt.hashpw(user_data.password.encode(), bcrypt.gensalt())
    
    # ユーザー作成
    user = User(
        email=user_data.email,
        username=user_data.username,
        password_hash=password_hash.decode()
    )
    
    db.add(user)
    db.commit()
    
    # 確認メール送信（非同期）
    send_verification_email.delay(user.id, user.email)
    
    return {"message": "User registered successfully", "user_id": user.id}

@app.post("/auth/login")
async def login_user(credentials: LoginSchema):
    """ログイン"""
    user = db.query(User).filter(User.email == credentials.email).first()
    
    if not user or not bcrypt.checkpw(credentials.password.encode(), user.password_hash.encode()):
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    # JWT生成
    token = jwt.encode({
        "user_id": user.id,
        "email": user.email,
        "exp": datetime.utcnow() + timedelta(hours=24)
    }, SECRET_KEY, algorithm="HS256")
    
    # ログイン履歴更新
    user.last_login = datetime.utcnow()
    db.commit()
    
    return {
        "access_token": token,
        "token_type": "bearer",
        "expires_in": 86400
    }
```

#### **2. Learning Service（学習セッション・進捗管理）**

```python
# learning_service/models.py
class LearningSession(Base):
    __tablename__ = 'learning_sessions'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, nullable=False)  # User Service参照
    session_type = Column(String(20))  # practice, test, review, challenge
    exam_type = Column(String(10), default='FE')
    category_filter = Column(JSON)  # 分野フィルター
    difficulty_range = Column(JSON)  # 難易度範囲
    
    total_questions = Column(Integer)
    correct_answers = Column(Integer, default=0)
    
    start_time = Column(DateTime, default=datetime.utcnow)
    end_time = Column(DateTime)
    duration_seconds = Column(Integer)
    
    # AI推奨フラグ
    is_ai_recommended = Column(Boolean, default=False)
    recommendation_reason = Column(String(255))
    
class LearningRecord(Base):
    __tablename__ = 'learning_records'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(Integer, ForeignKey('learning_sessions.id'))
    user_id = Column(Integer, nullable=False)
    question_id = Column(Integer, nullable=False)  # Question Service参照
    
    user_answer = Column(Integer)
    is_correct = Column(Boolean)
    confidence_level = Column(Integer)  # 1-5の自信度
    response_time_ms = Column(Integer)
    
    # 学習コンテキスト
    attempt_number = Column(Integer, default=1)  # 同問題の試行回数
    help_used = Column(Boolean, default=False)  # ヒント使用有無
    bookmarked = Column(Boolean, default=False)  # ブックマーク
    
    created_at = Column(DateTime, default=datetime.utcnow)

# learning_service/ai_engine.py
class AdaptiveLearningEngine:
    def __init__(self):
        self.difficulty_model = self.load_difficulty_model()
        self.recommendation_model = self.load_recommendation_model()
    
    async def get_next_questions(self, user_id: int, session_config: dict) -> List[int]:
        """AIによる次の問題推奨"""
        # ユーザーの学習履歴取得
        user_history = await self.get_user_learning_history(user_id)
        
        # 現在のスキルレベル推定
        skill_level = self.estimate_skill_level(user_history)
        
        # 弱点分野特定
        weak_areas = self.identify_weak_areas(user_history)
        
        # 最適問題選出
        if session_config.get('focus_on_weakness', True):
            questions = await self.select_weakness_focused_questions(
                weak_areas, skill_level, session_config
            )
        else:
            questions = await self.select_balanced_questions(
                skill_level, session_config
            )
        
        return questions
    
    def estimate_skill_level(self, history: List[LearningRecord]) -> Dict[str, float]:
        """分野別スキルレベル推定（Eloレーティング）"""
        skill_levels = {}
        
        for category in self.get_all_categories():
            category_records = [r for r in history if r.question.category == category]
            
            if not category_records:
                skill_levels[category] = 1200  # デフォルト値
                continue
            
            # Eloレーティング計算
            current_rating = 1200
            for record in sorted(category_records, key=lambda x: x.created_at):
                question_difficulty = record.question.difficulty_rating
                expected_score = 1 / (1 + 10**((question_difficulty - current_rating) / 400))
                actual_score = 1 if record.is_correct else 0
                
                # K因子（学習率）は試行回数に応じて調整
                k_factor = max(32 - len(category_records) // 10, 16)
                current_rating += k_factor * (actual_score - expected_score)
            
            skill_levels[category] = current_rating
        
        return skill_levels
```

#### **3. Question Service（問題データ管理・検索）**

```python
# question_service/models.py
class Question(Base):
    __tablename__ = 'questions'
    
    id = Column(Integer, primary_key=True)
    content_hash = Column(String(64), unique=True)  # 重複検出用
    
    # 問題内容
    question_text = Column(Text, nullable=False)
    choices = Column(JSON, nullable=False)  # 選択肢配列
    correct_answer = Column(Integer, nullable=False)
    explanation = Column(Text)
    
    # 分類情報
    exam_type = Column(String(10), nullable=False)  # FE, AP, IP, SG
    category = Column(String(100), nullable=False)
    subcategory = Column(String(100))
    keywords = Column(JSON)  # 検索用キーワード
    
    # 難易度・重要度
    difficulty_level = Column(Integer, default=2)  # 1-5
    importance_score = Column(Float, default=1.0)  # 出題頻度ベース
    difficulty_rating = Column(Float, default=1200)  # Eloレーティング
    
    # メタデータ
    year = Column(Integer)
    source = Column(String(100))  # 出典
    version = Column(Integer, default=1)
    
    # 関連性
    related_questions = Column(JSON)  # 関連問題ID配列
    prerequisite_concepts = Column(JSON)  # 前提概念
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow)

# question_service/search.py
from elasticsearch import Elasticsearch

class QuestionSearchEngine:
    def __init__(self):
        self.es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])
        self.setup_indices()
    
    def setup_indices(self):
        """Elasticsearchインデックス設定"""
        mapping = {
            "mappings": {
                "properties": {
                    "question_text": {
                        "type": "text",
                        "analyzer": "kuromoji"  # 日本語解析
                    },
                    "explanation": {
                        "type": "text", 
                        "analyzer": "kuromoji"
                    },
                    "category": {"type": "keyword"},
                    "exam_type": {"type": "keyword"},
                    "difficulty_level": {"type": "integer"},
                    "keywords": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        self.es.indices.create(index="questions", body=mapping, ignore=400)
    
    async def search_questions(self, query: QuestionSearchQuery) -> List[Dict]:
        """高度な問題検索"""
        search_body = {
            "query": {
                "bool": {
                    "must": [],
                    "filter": [],
                    "should": []
                }
            },
            "sort": [],
            "highlight": {
                "fields": {
                    "question_text": {},
                    "explanation": {}
                }
            }
        }
        
        # テキスト検索
        if query.text:
            search_body["query"]["bool"]["must"].append({
                "multi_match": {
                    "query": query.text,
                    "fields": ["question_text^2", "explanation", "keywords"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            })
        
        # フィルター条件
        if query.exam_type:
            search_body["query"]["bool"]["filter"].append({
                "term": {"exam_type": query.exam_type}
            })
        
        if query.categories:
            search_body["query"]["bool"]["filter"].append({
                "terms": {"category": query.categories}
            })
        
        if query.difficulty_range:
            search_body["query"]["bool"]["filter"].append({
                "range": {
                    "difficulty_level": {
                        "gte": query.difficulty_range.min,
                        "lte": query.difficulty_range.max
                    }
                }
            })
        
        # ソート
        if query.sort_by == "relevance":
            search_body["sort"] = ["_score"]
        elif query.sort_by == "difficulty":
            search_body["sort"] = [{"difficulty_level": {"order": "asc"}}]
        elif query.sort_by == "newest":
            search_body["sort"] = [{"created_at": {"order": "desc"}}]
        
        response = self.es.search(
            index="questions",
            body=search_body,
            size=query.limit,
            from_=query.offset
        )
        
        return self.format_search_results(response)
```

#### **4. Analytics Service（学習分析・レポート生成）**

```python
# analytics_service/models.py
class LearningAnalytics(Base):
    __tablename__ = 'learning_analytics'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, nullable=False)
    date = Column(Date, nullable=False)
    
    # 日次統計
    total_questions = Column(Integer, default=0)
    correct_answers = Column(Integer, default=0)
    study_time_minutes = Column(Integer, default=0)
    sessions_count = Column(Integer, default=0)
    
    # 分野別詳細（JSON）
    category_performance = Column(JSON)
    difficulty_distribution = Column(JSON)
    response_time_stats = Column(JSON)
    
    # 学習品質指標
    focus_score = Column(Float)  # 集中度（セッション時間/総時間）
    consistency_score = Column(Float)  # 一貫性（正答率の安定性）
    improvement_rate = Column(Float)  # 改善率（前日比）
    
    created_at = Column(DateTime, default=datetime.utcnow)

# analytics_service/reports.py
class ReportGenerator:
    def __init__(self):
        self.db = get_database_connection()
        self.ml_engine = MachineLearningEngine()
    
    async def generate_weekly_report(self, user_id: int) -> Dict:
        """週次学習レポート生成"""
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=7)
        
        # 基本統計取得
        analytics = await self.get_analytics_data(user_id, start_date, end_date)
        
        # 前週比較
        prev_week_analytics = await self.get_analytics_data(
            user_id, start_date - timedelta(days=7), start_date
        )
        
        # 学習パターン分析
        learning_patterns = await self.analyze_learning_patterns(user_id, analytics)
        
        # AI による改善提案
        recommendations = await self.ml_engine.generate_recommendations(user_id, analytics)
        
        # 弱点・強み分析
        strengths_weaknesses = await self.analyze_strengths_weaknesses(user_id, analytics)
        
        return {
            "period": {"start": start_date, "end": end_date},
            "summary": {
                "total_questions": sum(a.total_questions for a in analytics),
                "accuracy_rate": self.calculate_accuracy_rate(analytics),
                "study_time": sum(a.study_time_minutes for a in analytics),
                "improvement": self.calculate_improvement(analytics, prev_week_analytics)
            },
            "daily_breakdown": self.format_daily_breakdown(analytics),
            "category_performance": self.analyze_category_performance(analytics),
            "learning_patterns": learning_patterns,
            "recommendations": recommendations,
            "strengths_weaknesses": strengths_weaknesses,
            "goals": await self.check_goal_progress(user_id, analytics)
        }
    
    async def analyze_learning_patterns(self, user_id: int, analytics: List) -> Dict:
        """学習パターンの分析"""
        patterns = {
            "peak_hours": [],
            "optimal_session_length": 0,
            "consistency_score": 0,
            "learning_velocity": 0
        }
        
        # 時間帯別パフォーマンス分析
        hourly_performance = {}
        sessions = await self.get_user_sessions(user_id, days=30)
        
        for session in sessions:
            hour = session.start_time.hour
            if hour not in hourly_performance:
                hourly_performance[hour] = []
            
            accuracy = session.correct_answers / session.total_questions if session.total_questions > 0 else 0
            hourly_performance[hour].append(accuracy)
        
        # 最適時間帯特定
        peak_performance = {}
        for hour, performances in hourly_performance.items():
            if len(performances) >= 3:  # 最低3セッション
                peak_performance[hour] = np.mean(performances)
        
        if peak_performance:
            best_hours = sorted(peak_performance.items(), key=lambda x: x[1], reverse=True)[:3]
            patterns["peak_hours"] = [{"hour": h, "accuracy": acc} for h, acc in best_hours]
        
        # 最適セッション時間分析
        session_lengths = [(s.duration_seconds // 60, s.correct_answers / s.total_questions) 
                          for s in sessions if s.duration_seconds and s.total_questions > 0]
        
        if session_lengths:
            # セッション時間とパフォーマンスの相関
            duration_groups = {}
            for duration, accuracy in session_lengths:
                group = (duration // 15) * 15  # 15分単位でグループ化
                if group not in duration_groups:
                    duration_groups[group] = []
                duration_groups[group].append(accuracy)
            
            best_duration = max(duration_groups.items(), 
                              key=lambda x: np.mean(x[1]) if len(x[1]) >= 2 else 0)
            patterns["optimal_session_length"] = best_duration[0]
        
        return patterns
```

---

## 🚀 開発・運用改善提案

### DevOps Pipeline 強化

#### **CI/CD Pipeline（GitHub Actions）**

```yaml
# .github/workflows/comprehensive-pipeline.yml
name: Comprehensive CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: it-exam-learning-system

jobs:
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Lint with flake8
        run: |
          flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      
      - name: Format check with black
        run: black --check src/
      
      - name: Type check with mypy
        run: mypy src/ --ignore-missing-imports
      
      - name: Security check with bandit
        run: bandit -r src/ -f json -o bandit-report.json
      
      - name: Upload security report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json

  unit-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run unit tests
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 30  # サービス起動待ち
      
      - name: Run integration tests
        run: |
          pytest tests/integration/ -v --maxfail=5
      
      - name: Cleanup
        if: always()
        run: docker-compose -f docker-compose.test.yml down

  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install Playwright
        run: |
          npm install -g @playwright/test
          playwright install
      
      - name: Start application
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 60  # アプリケーション起動待ち
      
      - name: Run E2E tests
        run: |
          playwright test tests/e2e/
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: playwright-report/

  security-scan:
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

  build-and-push:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./Dockerfile.prod
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
      - name: Deploy to staging
        run: |
          curl -X POST "${{ secrets.STAGING_DEPLOY_WEBHOOK }}" \
            -H "Content-Type: application/json" \
            -d '{"ref": "${{ github.sha }}"}'

  deploy-production:
    runs-on: ubuntu-latest
    needs: [build-and-push, e2e-tests]
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Deploy to production
        run: |
          curl -X POST "${{ secrets.PRODUCTION_DEPLOY_WEBHOOK }}" \
            -H "Content-Type: application/json" \
            -d '{"ref": "${{ github.sha }}"}'
      
      - name: Run smoke tests
        run: |
          sleep 120  # デプロイ完了待ち
          curl -f https://it-exam-learning-system.com/health || exit 1
          
      - name: Notify deployment
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### Infrastructure as Code

#### **Terraform Configuration**

```hcl
# infrastructure/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "it-exam-terraform-state"
    key    = "prod/terraform.tfstate"
    region = "ap-northeast-1"
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC Configuration
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 3.0"
  
  name = "it-exam-vpc"
  cidr = "10.0.0.0/16"
  
  azs             = ["ap-northeast-1a", "ap-northeast-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]
  
  enable_nat_gateway = true
  enable_vpn_gateway = false
  
  tags = local.common_tags
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "it-exam-cluster"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
  
  tags = local.common_tags
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "it-exam-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets           = module.vpc.public_subnets
  
  enable_deletion_protection = false
  
  tags = local.common_tags
}

# RDS PostgreSQL
resource "aws_db_instance" "main" {
  identifier = "it-exam-db"
  
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = "db.t3.micro"
  
  allocated_storage     = 20
  max_allocated_storage = 100
  storage_type         = "gp2"
  storage_encrypted    = true
  
  db_name  = "learning_system"
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = true
  deletion_protection = false
  
  performance_insights_enabled = true
  monitoring_interval         = 60
  monitoring_role_arn        = aws_iam_role.rds_monitoring.arn
  
  tags = local.common_tags
}

# ElastiCache Redis
resource "aws_elasticache_subnet_group" "main" {
  name       = "it-exam-cache-subnet"
  subnet_ids = module.vpc.private_subnets
}

resource "aws_elasticache_replication_group" "main" {
  replication_group_id         = "it-exam-redis"
  description                  = "Redis cluster for IT Exam Learning System"
  
  port               = 6379
  parameter_group_name = "default.redis7"
  node_type          = "cache.t3.micro"
  num_cache_clusters = 2
  
  subnet_group_name  = aws_elasticache_subnet_group.main.name
  security_group_ids = [aws_security_group.redis.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  
  tags = local.common_tags
}

# CloudWatch Log Groups
resource "aws_cloudwatch_log_group" "app" {
  name              = "/ecs/it-exam-app"
  retention_in_days = 30
  
  tags = local.common_tags
}

# Local values
locals {
  common_tags = {
    Project     = "IT Exam Learning System"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}
```

### Monitoring & Observability

#### **Prometheus + Grafana 設定**

```yaml
# monitoring/docker-compose.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:
```

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
  
  - job_name: 'it-exam-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
  
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
```

#### **アプリケーションメトリクス**

```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from flask import Response
import time
import psutil

# カスタムメトリクス定義
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_USERS = Gauge(
    'active_users_total',
    'Number of active users'
)

LEARNING_SESSIONS = Counter(
    'learning_sessions_total',
    'Total learning sessions',
    ['exam_type', 'session_type']
)

QUESTION_RESPONSES = Counter(
    'question_responses_total',
    'Total question responses',
    ['category', 'is_correct']
)

DATABASE_CONNECTIONS = Gauge(
    'database_connections_active',
    'Active database connections'
)

SYSTEM_MEMORY_USAGE = Gauge(
    'system_memory_usage_bytes',
    'System memory usage in bytes'
)

class MetricsCollector:
    def __init__(self, app):
        self.app = app
        self.setup_middleware()
        self.setup_metrics_endpoint()
    
    def setup_middleware(self):
        @self.app.before_request
        def before_request():
            request.start_time = time.time()
        
        @self.app.after_request
        def after_request(response):
            # リクエスト時間記録
            duration = time.time() - request.start_time
            REQUEST_DURATION.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown'
            ).observe(duration)
            
            # リクエスト数記録
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown',
                status=response.status_code
            ).inc()
            
            return response
    
    def setup_metrics_endpoint(self):
        @self.app.route('/metrics')
        def metrics():
            # システムメトリクス更新
            self.update_system_metrics()
            
            # Prometheusフォーマットで出力
            return Response(
                generate_latest(),
                mimetype='text/plain'
            )
    
    def update_system_metrics(self):
        # メモリ使用量
        memory = psutil.virtual_memory()
        SYSTEM_MEMORY_USAGE.set(memory.used)
        
        # アクティブユーザー数（Redis から取得）
        active_users = self.get_active_users_count()
        ACTIVE_USERS.set(active_users)
        
        # データベース接続数
        db_connections = self.get_db_connections_count()
        DATABASE_CONNECTIONS.set(db_connections)
    
    def record_learning_activity(self, exam_type: str, session_type: str):
        """学習活動の記録"""
        LEARNING_SESSIONS.labels(
            exam_type=exam_type,
            session_type=session_type
        ).inc()
    
    def record_question_response(self, category: str, is_correct: bool):
        """問題回答の記録"""
        QUESTION_RESPONSES.labels(
            category=category,
            is_correct=str(is_correct).lower()
        ).inc()
```

---

## 📊 実装ロードマップ

### Phase 1: 基盤強化（4-6週間）

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u73fe\u72b6\u5206\u6790\u3068\u30e6\u30fc\u30b6\u30fc\u30cb\u30fc\u30ba\u8abf\u67fb", "status": "completed", "priority": "high", "id": "1"}, {"content": "\u6a5f\u80fd\u8981\u4ef6\u306e\u8a73\u7d30\u5b9a\u7fa9", "status": "completed", "priority": "high", "id": "2"}, {"content": "\u6280\u8853\u8981\u4ef6\u3068\u5236\u7d04\u4e8b\u9805\u306e\u6574\u7406", "status": "completed", "priority": "high", "id": "3"}, {"content": "UI/UX\u6539\u5584\u63d0\u6848", "status": "completed", "priority": "medium", "id": "4"}, {"content": "\u30b7\u30b9\u30c6\u30e0\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u5f37\u5316\u6848", "status": "completed", "priority": "medium", "id": "5"}, {"content": "\u958b\u767a\u30fb\u904b\u7528\u6539\u5584\u63d0\u6848", "status": "in_progress", "priority": "medium", "id": "6"}, {"content": "\u5b9f\u88c5\u512a\u5148\u5ea6\u3068\u30ed\u30fc\u30c9\u30de\u30c3\u30d7\u4f5c\u6210", "status": "pending", "priority": "high", "id": "7"}]